{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.bencmark = True\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys,cv2,random,datetime,time,math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from net_s3fd import *\n",
    "from s3fd import *\n",
    "from bbox import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['Image_Name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['Image_Name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['Gender'].str.split()).astype(np.float32)\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        #img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        #img = Variable(torch.from_numpy(img).float(),volatile=True)\n",
    "        \n",
    "        #if self.transform is not None:\n",
    "        #    img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor()\n",
    "     \n",
    "     #transforms.Normalize(mean=[104,117,123])\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"index.csv\"\n",
    "img_path = \"data/Celeb_Small_Dataset/\"\n",
    "img_ext = \".jpg\"\n",
    "dset = CelebDataset(train_data,img_path,img_ext,transformations)\n",
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 1.,  0.])\n",
      "torch.Size([2])\n",
      "torch.Size([23, 2])\n",
      "tensor([[ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  0.]])\n",
      "(256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## bad data visualization, perhaps need to change from BGR to RGB\n",
    "fig = plt.figure()\n",
    "for i in range(len(dset)):\n",
    "    sample = dset[0]\n",
    "    print(i)\n",
    "    print(sample[1])\n",
    "    image = sample[0]\n",
    "    tensor_result = sample[1]\n",
    "    print(sample[1].size())\n",
    "    tensor_result = tensor_result.unsqueeze(0)\n",
    "#     tensor_result = tensor_result.unsqueeze(2)\n",
    "#     tensor_result = tensor_result.unsqueeze(3)\n",
    "    tensor_result = tensor_result.expand(23,2)\n",
    "    print(tensor_result.size())\n",
    "    print(tensor_result)\n",
    "#     tensor_repeated = tensor_result.\n",
    "    image_changed = image.permute(1,2,0)\n",
    "    print(image_changed.numpy().shape)\n",
    "#     print(image.transpose().shape)\n",
    "#     image_changed = [image[1], image[2], image[0]]\n",
    "    plt.imshow(image_changed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, optimizer, loss, filename):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.data[0]\n",
    "        }\n",
    "    torch.save(save_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_classes, num_epochs = 100):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i,(img,label) in enumerate(train_loader):\n",
    "            img = img.view((1,)+img.shape[1:])\n",
    "            if use_cuda:\n",
    "                data, target = Variable(img.cuda()), Variable(torch.Tensor(label).cuda())\n",
    "            else:\n",
    "                data, target = Variable(img), Variable(torch.Tensor(label))\n",
    "            target = target.view(1, num_classes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            gender_classifications = model(data, use_cuda)\n",
    "#             for gender_classification in gender_classifications:\n",
    "#                 FB, FC, FH, FW = gender_classification.size()\n",
    "#                 tensor_target = target.unsqueeze(2)\n",
    "#                 tensor_target = tensor_target.unsqueeze(3)\n",
    "#                 tensor_target = tensor_target.expand(1, num_classes, FH, FW)\n",
    "#                 assert(gender_classification.size() == tensor_target.size())\n",
    "#                 loss = criterion(gender_classification, tensor_target)\n",
    "#                 running_loss += loss.item()\n",
    "#                 loss.backward()\n",
    "            tensor_target = target.expand(gender_classifications.size()[0], num_classes)\n",
    "            loss = criterion(gender_classifications, tensor_target)\n",
    "            print(loss.item())\n",
    "            loss.backward()\n",
    "                        \n",
    "            \n",
    "            for name, parameter in myModel.named_parameters():\n",
    "                if parameter.grad is not None:\n",
    "                    print(name)\n",
    "                    print(parameter.grad)\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        print(\"Loss = \", running_loss)\n",
    "        print('-' * 10)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            save(model, optimizer, loss, 'faceRecog.saved.model')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the new model\n",
    "num_classes = 2\n",
    "myModel = s3fd_original()\n",
    "\n",
    "# load pre trained weights\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "newModel = myModel.state_dict()\n",
    "\n",
    "# compute intersection\n",
    "pretrained_dict = {k: v for k, v in loadedModel.items() if k in newModel}\n",
    "\n",
    "# update newModel dictionary\n",
    "newModel.update(pretrained_dict)\n",
    "#update the pytorch model\n",
    "myModel.load_state_dict(newModel)\n",
    "\n",
    "# freeze all layers\n",
    "for param in myModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze gender layer weights and biases\n",
    "\n",
    "myModel.conv4_3_norm_gender.weight.requires_grad = True\n",
    "myModel.conv4_3_norm_gender.bias.requires_grad = True\n",
    "\n",
    "myModel.conv5_3_norm_gender.weight.requires_grad = True\n",
    "myModel.conv5_3_norm_gender.bias.requires_grad = True\n",
    "\n",
    "myModel.conv6_2_gender.weight.requires_grad = True\n",
    "myModel.conv6_2_gender.bias.requires_grad = True\n",
    "\n",
    "myModel.fc7_gender.weight.requires_grad = True\n",
    "myModel.fc7_gender.bias.requires_grad = True\n",
    "\n",
    "# now lets initialize the new layers with the face weights from previous layers\n",
    "\n",
    "conv4_3_tensor = myModel.conv4_3_norm_mbox_conf.weight[0]\n",
    "conv4_3_tensor.unsqueeze_(0)\n",
    "conv_4_3_gen_weights = torch.cat((conv4_3_tensor, conv4_3_tensor), 0)\n",
    "myModel.conv4_3_norm_gender.weight = torch.nn.Parameter(conv_4_3_gen_weights)\n",
    "# torch.nn.init.xavier_uniform_(myModel.conv4_3_norm_gender.weight)\n",
    "\n",
    "\n",
    "conv5_3_tensor = myModel.conv5_3_norm_mbox_conf.weight[0]\n",
    "conv5_3_tensor.unsqueeze_(0)\n",
    "conv_5_3_gen_weights = torch.cat((conv5_3_tensor, conv5_3_tensor), 0)\n",
    "myModel.conv5_3_norm_gender.weight = torch.nn.Parameter(conv_5_3_gen_weights)\n",
    "# torch.nn.init.xavier_uniform_(myModel.conv5_3_norm_gender.weight)\n",
    "\n",
    "\n",
    "fc7_tensor = myModel.fc7_mbox_conf.weight[0]\n",
    "fc7_tensor.unsqueeze_(0)\n",
    "fc7_gen_weights = torch.cat((fc7_tensor, fc7_tensor), 0)\n",
    "myModel.fc7_gender.weight = torch.nn.Parameter(fc7_gen_weights)\n",
    "# torch.nn.init.xavier_uniform_(myModel.fc7_gender.weight)\n",
    "\n",
    "conv6_2_tensor = myModel.conv6_2_mbox_conf.weight[0].unsqueeze_(0)\n",
    "myModel.conv6_2_gender.weight = torch.nn.Parameter(torch.cat((conv6_2_tensor, conv6_2_tensor), 0))\n",
    "# torch.nn.init.xavier_uniform_(myModel.conv6_2_gender.weight)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3fd_original(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
       "  (fc7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv7_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv7_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_3_norm): L2Norm()\n",
       "  (conv4_3_norm): L2Norm()\n",
       "  (conv5_3_norm): L2Norm()\n",
       "  (conv3_3_norm_mbox_conf): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3_norm_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_conf): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_loc): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_gender): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_conf): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "Epoch 1/100\n",
      "0.6929264664649963\n",
      "0.6970528364181519\n",
      "0.689474880695343\n",
      "0.6951481699943542\n",
      "0.688607394695282\n",
      "0.6983307003974915\n",
      "0.6884839534759521\n",
      "0.6983307003974915\n",
      "0.6972271800041199\n",
      "0.6926637291908264\n",
      "0.6934364438056946\n",
      "0.6929264664649963\n",
      "0.6922212839126587\n",
      "0.6960101127624512\n",
      "0.6913558840751648\n",
      "0.6936051845550537\n",
      "0.6983306407928467\n",
      "0.694133460521698\n",
      "0.6937337517738342\n",
      "0.693487823009491\n",
      "0.691192090511322\n",
      "0.6983305811882019\n",
      "0.697269856929779\n",
      "0.6946178078651428\n",
      "0.6942802667617798\n",
      "0.6946178078651428\n",
      "0.6949552893638611\n",
      "0.6922212839126587\n",
      "0.6915314793586731\n",
      "0.697269856929779\n",
      "0.6879903078079224\n",
      "0.692702054977417\n",
      "0.6967673897743225\n",
      "0.6983306407928467\n",
      "0.6947115063667297\n",
      "0.6983307003974915\n",
      "0.6915866136550903\n",
      "0.6983306407928467\n",
      "0.6935039162635803\n",
      "0.6932499408721924\n",
      "0.6983307003974915\n",
      "0.6923691034317017\n",
      "0.693487823009491\n",
      "0.6969555020332336\n",
      "0.6912810802459717\n",
      "0.6929841637611389\n",
      "0.6946178674697876\n",
      "0.697269856929779\n",
      "0.6973159313201904\n",
      "0.6927614212036133\n",
      "0.6879903674125671\n",
      "0.6879903078079224\n",
      "0.6968930959701538\n",
      "0.6936051845550537\n",
      "0.6983306407928467\n",
      "0.6983307600021362\n",
      "0.6983306407928467\n",
      "0.6915161609649658\n",
      "0.6968455910682678\n",
      "0.6934753656387329\n",
      "0.6882489323616028\n",
      "0.6946178078651428\n",
      "0.6924327611923218\n",
      "0.6940857172012329\n",
      "0.6920165419578552\n",
      "0.6983306407928467\n",
      "0.688766360282898\n",
      "0.6940315365791321\n",
      "0.6930386424064636\n",
      "0.6979199647903442\n",
      "0.6938106417655945\n",
      "0.6932676434516907\n",
      "0.6914746165275574\n",
      "0.6983307003974915\n",
      "0.6967333555221558\n",
      "0.6940873861312866\n",
      "0.6905921697616577\n",
      "0.6941335201263428\n",
      "0.6886634230613708\n",
      "0.6948131918907166\n",
      "0.6983306407928467\n",
      "0.692702054977417\n",
      "0.6942464709281921\n",
      "0.6983306407928467\n",
      "0.6940874457359314\n",
      "0.6936051845550537\n",
      "0.6944223642349243\n",
      "0.6975057125091553\n",
      "0.6946178078651428\n",
      "0.6983307003974915\n",
      "0.6896990537643433\n",
      "0.6983306407928467\n",
      "0.6925738453865051\n",
      "0.6912296414375305\n",
      "0.6971805691719055\n",
      "0.6967909932136536\n",
      "0.6939975619316101\n",
      "0.6938106417655945\n",
      "0.6961401700973511\n",
      "0.6937337517738342\n",
      "0.6952040791511536\n",
      "0.697084903717041\n",
      "0.6983307003974915\n",
      "0.693942666053772\n",
      "0.6941338777542114\n",
      "0.6931940913200378\n",
      "0.6983306407928467\n",
      "0.6928431987762451\n",
      "0.6925569772720337\n",
      "0.6938751935958862\n",
      "0.693487823009491\n",
      "0.6935039758682251\n",
      "0.6968930959701538\n",
      "0.6939427256584167\n",
      "0.6937491297721863\n",
      "0.6944563388824463\n",
      "0.6983306407928467\n",
      "0.6938751935958862\n",
      "0.6927613615989685\n",
      "0.6983307003974915\n",
      "0.697269856929779\n",
      "0.6967909932136536\n",
      "0.6968930959701538\n",
      "0.6946328282356262\n",
      "0.693942666053772\n",
      "0.6968496441841125\n",
      "0.6894212961196899\n",
      "0.6937501430511475\n",
      "0.6932094097137451\n",
      "0.6937337517738342\n",
      "0.6970527768135071\n",
      "0.6925738453865051\n",
      "0.6946178078651428\n",
      "0.6921696662902832\n",
      "0.6978496313095093\n",
      "0.6924327611923218\n",
      "0.6929264664649963\n",
      "0.6909520626068115\n",
      "0.696491003036499\n",
      "0.6938106417655945\n",
      "0.6967394948005676\n",
      "0.6970528364181519\n",
      "0.6968353986740112\n",
      "0.6936895847320557\n",
      "0.6916924118995667\n",
      "0.6983307003974915\n",
      "0.6983307003974915\n",
      "0.6983306407928467\n",
      "0.6938751935958862\n",
      "0.6923655271530151\n",
      "0.6950394511222839\n",
      "0.6975489854812622\n",
      "0.6974165439605713\n",
      "0.6983307600021362\n",
      "0.6929264664649963\n",
      "0.6923655271530151\n",
      "0.6937623620033264\n",
      "0.694512665271759\n",
      "0.6884042024612427\n",
      "0.6879903674125671\n",
      "0.6946178078651428\n",
      "0.6983306407928467\n",
      "0.6940165162086487\n",
      "0.6936051845550537\n",
      "0.6924327611923218\n",
      "0.692702054977417\n",
      "0.6946178078651428\n",
      "0.693487823009491\n",
      "0.6943367719650269\n",
      "0.6922382116317749\n",
      "0.692497193813324\n",
      "0.6892622709274292\n",
      "0.6983306407928467\n",
      "0.6925738453865051\n",
      "0.6949890851974487\n",
      "0.6983307600021362\n",
      "0.6938106417655945\n",
      "0.6934877634048462\n",
      "0.6913558840751648\n",
      "0.6946178078651428\n",
      "0.6970930099487305\n",
      "0.6954189538955688\n",
      "0.6967394948005676\n",
      "0.6926298141479492\n",
      "0.6937351226806641\n",
      "0.6929264068603516\n",
      "0.6951674222946167\n",
      "0.6949552893638611\n",
      "0.6934225559234619\n",
      "0.6971200704574585\n",
      "0.6968930959701538\n",
      "0.698070228099823\n",
      "0.6928030252456665\n",
      "0.6940315365791321\n",
      "0.6920664310455322\n",
      "0.697509229183197\n",
      "0.6940873861312866\n",
      "0.6879903078079224\n",
      "0.692497193813324\n",
      "Loss =  0.0\n",
      "----------\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda2/envs/deep_learning/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6940315365791321\n",
      "0.6983307003974915\n",
      "0.6940315365791321\n",
      "0.6983307003974915\n",
      "0.6937623620033264\n",
      "0.697269856929779\n",
      "0.6950394511222839\n",
      "0.6983306407928467\n",
      "0.6905921697616577\n",
      "0.688607394695282\n",
      "0.6970528364181519\n",
      "0.6983307003974915\n",
      "0.6940873861312866\n",
      "0.6968353986740112\n",
      "0.6913558840751648\n",
      "0.6929841637611389\n",
      "0.6879903674125671\n",
      "0.6882489323616028\n",
      "0.6923655271530151\n",
      "0.6942464709281921\n",
      "0.6916924118995667\n",
      "0.6932499408721924\n",
      "0.6983306407928467\n",
      "0.6983306407928467\n",
      "0.6971805691719055\n",
      "0.693487823009491\n",
      "0.6983306407928467\n",
      "0.6975489854812622\n",
      "0.693487823009491\n",
      "0.694512665271759\n",
      "0.6968496441841125\n",
      "0.6948131918907166\n",
      "0.6936051845550537\n",
      "0.6934753656387329\n",
      "0.6929264068603516\n",
      "0.6983307003974915\n",
      "0.6946178078651428\n",
      "0.6983306407928467\n",
      "0.693942666053772\n",
      "0.6960101127624512\n",
      "0.6983306407928467\n",
      "0.6884839534759521\n",
      "0.6949552893638611\n",
      "0.6944563388824463\n",
      "0.6944223642349243\n",
      "0.6937337517738342\n",
      "0.6922382116317749\n",
      "0.6937501430511475\n",
      "0.6983306407928467\n",
      "0.6940873861312866\n",
      "0.6935039162635803\n",
      "0.6946178078651428\n",
      "0.6920165419578552\n",
      "0.689474880695343\n",
      "0.697509229183197\n",
      "0.696491003036499\n",
      "0.6936895847320557\n",
      "0.691192090511322\n",
      "0.6970527768135071\n",
      "0.6920664310455322\n",
      "0.6927614212036133\n",
      "0.6940874457359314\n",
      "0.6983305811882019\n",
      "0.6879903078079224\n",
      "0.6929264664649963\n",
      "0.6937351226806641\n",
      "0.6983306407928467\n",
      "0.6931940913200378\n",
      "0.6983307003974915\n",
      "0.6946328282356262\n",
      "0.6924327611923218\n",
      "0.6937337517738342\n",
      "0.6983306407928467\n",
      "0.6970930099487305\n",
      "0.6967909932136536\n",
      "0.692702054977417\n",
      "0.6938751935958862\n",
      "0.693942666053772\n",
      "0.6983306407928467\n",
      "0.6946178078651428\n",
      "0.6983307003974915\n",
      "0.6925569772720337\n",
      "0.6932094097137451\n",
      "0.6983307003974915\n",
      "0.6924327611923218\n",
      "0.697269856929779\n",
      "0.6968930959701538\n",
      "0.697084903717041\n",
      "0.6949552893638611\n",
      "0.6968455910682678\n",
      "0.6915866136550903\n",
      "0.6946178078651428\n",
      "0.6921696662902832\n",
      "0.6973159313201904\n",
      "0.688766360282898\n",
      "0.6983307600021362\n",
      "0.6961401700973511\n",
      "0.6924327611923218\n",
      "0.6879903078079224\n",
      "0.6894212961196899\n",
      "0.6951481699943542\n",
      "0.697269856929779\n",
      "0.6946178078651428\n",
      "0.6974165439605713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/parth/anaconda2/envs/deep_learning/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934225559234619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-601aeb829a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f12a4e8bfe4b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_classes, num_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mgender_classifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#             for gender_classification in gender_classifications:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#                 FB, FC, FH, FW = gender_classification.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ComputerVision_FinalProj/s3fd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, use_cuda)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_class_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mscore_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_class_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mgender_score_female\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgender_class_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mgender_score_male\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgender_class_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "# turn off gradient update for old layers\n",
    "# for param in myModel.parameters():\n",
    "#     param.requires_grad = False\n",
    "# myModel.conv3_3_norm_gender.weight.requires_grad = True\n",
    "# myModel.conv3_3_norm_gender.bias.requires_grad = True\n",
    "\n",
    "# myModel.fc_1 = nn.Linear(2304,num_classes)\n",
    "# for parameter in myModel.parameters():\n",
    "#     print(parameter.requires_grad)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.001, momentum=0.9)\n",
    "if use_cuda:\n",
    "    print(\"using cuda\")\n",
    "    myModel = myModel.cuda()\n",
    "model_ft = train_model(myModel, criterion, optimizer, num_classes, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(filter(lambda p: p.requires_grad,myModel.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        return Variable(img.cuda())\n",
    "myModel = myModel.cuda()\n",
    "testImage1 = transform('data/Test/TestCeleb_4/25-FaceId-0.jpg')\n",
    "testImage2 = transform('data/Test/TestCeleb_4/26-FaceId-0.jpg')\n",
    "testImage3 = transform('data/Test/TestCeleb_4/27-FaceId-0.jpg')\n",
    "testImage4 = transform('data/Test/TestCeleb_10/25-FaceId-0.jpg')\n",
    "testImage5 = transform('data/Test/TestCeleb_10/26-FaceId-0.jpg')\n",
    "testImage6 = transform('data/Test/TestCeleb_10/24-FaceId-0.jpg')\n",
    "\n",
    "output1 = myModel(testImage1)\n",
    "output2 = myModel(testImage2)\n",
    "output3 = myModel(testImage2)\n",
    "output4 = myModel(testImage4)\n",
    "output5 = myModel(testImage5)\n",
    "output6 = myModel(testImage6)\n",
    "print(\"testImage1 - \",output1)\n",
    "print(\"testImage2 - \",output2)\n",
    "print(\"testImage3 - \",output3)\n",
    "print(\"testImage1 - \",output4)\n",
    "print(\"testImage2 - \",output5)\n",
    "print(\"testImage3 - \",output6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in myModel.parameters():\n",
    "    print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
