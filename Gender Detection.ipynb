{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.bencmark = True\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys,cv2,random,datetime,time,math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from net_s3fd import *\n",
    "from s3fd import *\n",
    "from bbox import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['Image_Name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['Image_Name']\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['Gender'].str.split()).astype(np.float32)\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        #img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        #img = Variable(torch.from_numpy(img).float(),volatile=True)\n",
    "        \n",
    "        #if self.transform is not None:\n",
    "        #    img = self.transform(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor()\n",
    "     \n",
    "     #transforms.Normalize(mean=[104,117,123])\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"index.csv\"\n",
    "img_path = \"data/Celeb_Small_Dataset/\"\n",
    "img_ext = \".jpg\"\n",
    "dset = CelebDataset(train_data,img_path,img_ext,transformations)\n",
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 1.,  0.])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "tensor([ 1.,  0.])\n",
      "(256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## bad data visualization, perhaps need to change from BGR to RGB\n",
    "fig = plt.figure()\n",
    "for i in range(len(dset)):\n",
    "    sample = dset[0]\n",
    "    print(i)\n",
    "    print(sample[1])\n",
    "    image = sample[0]\n",
    "    tensor_result = sample[1]\n",
    "    print(sample[1].size())\n",
    "#     tensor_result = tensor_result.unsqueeze(0)\n",
    "#     tensor_result = tensor_result.unsqueeze(2)\n",
    "#     tensor_result = tensor_result.unsqueeze(3)\n",
    "#     tensor_result = tensor_result.expand(23,2)\n",
    "    print(tensor_result.size())\n",
    "    print(tensor_result)\n",
    "#     tensor_repeated = tensor_result.\n",
    "    image_changed = image.permute(1,2,0)\n",
    "    print(image_changed.numpy().shape)\n",
    "#     print(image.transpose().shape)\n",
    "#     image_changed = [image[1], image[2], image[0]]\n",
    "    plt.imshow(image_changed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, optimizer, loss, filename):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.data[0]\n",
    "        }\n",
    "    torch.save(save_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the new model\n",
    "num_classes = 2\n",
    "myModel = s3fd(num_classes)\n",
    "\n",
    "# load pre trained weights\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "newModel = myModel.state_dict()\n",
    "\n",
    "# compute intersection\n",
    "pretrained_dict = {k: v for k, v in loadedModel.items() if k in newModel}\n",
    "\n",
    "# update newModel dictionary\n",
    "newModel.update(pretrained_dict)\n",
    "#update the pytorch model\n",
    "myModel.load_state_dict(newModel)\n",
    "\n",
    "# freeze all layers\n",
    "for param in myModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "myModel.fc_1.weight.requires_grad = True\n",
    "myModel.fc_1.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3fd(\n",
      "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
      "  (fc7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv6_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv6_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv7_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv7_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc_1): Linear(in_features=2304, out_features=2, bias=True)\n",
      ")\n",
      "conv1_1.weight\n",
      "False\n",
      "conv1_1.bias\n",
      "False\n",
      "conv1_2.weight\n",
      "False\n",
      "conv1_2.bias\n",
      "False\n",
      "conv2_1.weight\n",
      "False\n",
      "conv2_1.bias\n",
      "False\n",
      "conv2_2.weight\n",
      "False\n",
      "conv2_2.bias\n",
      "False\n",
      "conv3_1.weight\n",
      "False\n",
      "conv3_1.bias\n",
      "False\n",
      "conv3_2.weight\n",
      "False\n",
      "conv3_2.bias\n",
      "False\n",
      "conv3_3.weight\n",
      "False\n",
      "conv3_3.bias\n",
      "False\n",
      "conv4_1.weight\n",
      "False\n",
      "conv4_1.bias\n",
      "False\n",
      "conv4_2.weight\n",
      "False\n",
      "conv4_2.bias\n",
      "False\n",
      "conv4_3.weight\n",
      "False\n",
      "conv4_3.bias\n",
      "False\n",
      "conv5_1.weight\n",
      "False\n",
      "conv5_1.bias\n",
      "False\n",
      "conv5_2.weight\n",
      "False\n",
      "conv5_2.bias\n",
      "False\n",
      "conv5_3.weight\n",
      "False\n",
      "conv5_3.bias\n",
      "False\n",
      "fc6.weight\n",
      "False\n",
      "fc6.bias\n",
      "False\n",
      "fc7.weight\n",
      "False\n",
      "fc7.bias\n",
      "False\n",
      "conv6_1.weight\n",
      "False\n",
      "conv6_1.bias\n",
      "False\n",
      "conv6_2.weight\n",
      "False\n",
      "conv6_2.bias\n",
      "False\n",
      "conv7_1.weight\n",
      "False\n",
      "conv7_1.bias\n",
      "False\n",
      "conv7_2.weight\n",
      "False\n",
      "conv7_2.bias\n",
      "False\n",
      "fc_1.weight\n",
      "True\n",
      "fc_1.bias\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "print(myModel.eval())\n",
    "for name, parameter in myModel.named_parameters():\n",
    "    print(name)\n",
    "    print(parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_classes, num_epochs = 100):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i,(img,label) in enumerate(train_loader):\n",
    "            img = img.view((1,)+img.shape[1:])\n",
    "            if use_cuda:\n",
    "                data, target = Variable(img.cuda()), Variable(torch.Tensor(label).cuda())\n",
    "            else:\n",
    "                data, target = Variable(img), Variable(torch.Tensor(label))\n",
    "            target = target.view(1, num_classes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            gender_classifications = model(data)\n",
    "            loss = criterion(gender_classifications, target)\n",
    "            running_loss+= loss.item()\n",
    "            loss.backward()\n",
    "#             for name, parameter in model.named_parameters():\n",
    "#                 if parameter.grad is not None:\n",
    "#                     print(name)\n",
    "#                     print(parameter.grad)\n",
    "            optimizer.step()\n",
    "        print(\"Loss = \", running_loss)\n",
    "        print(\"-\" * 10)\n",
    "        losses.append(running_loss)\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "Epoch 1/100\n",
      "Loss =  128.51454934477806\n",
      "----------\n",
      "Epoch 2/100\n",
      "Loss =  129.85995699465275\n",
      "----------\n",
      "Epoch 3/100\n",
      "Loss =  126.42474375665188\n",
      "----------\n",
      "Epoch 4/100\n",
      "Loss =  125.72337248921394\n",
      "----------\n",
      "Epoch 5/100\n",
      "Loss =  124.87221898138523\n",
      "----------\n",
      "Epoch 6/100\n",
      "Loss =  128.29409576952457\n",
      "----------\n",
      "Epoch 7/100\n",
      "Loss =  126.23607462644577\n",
      "----------\n",
      "Epoch 8/100\n",
      "Loss =  131.12904205918312\n",
      "----------\n",
      "Epoch 9/100\n",
      "Loss =  123.43876995146275\n",
      "----------\n",
      "Epoch 10/100\n",
      "Loss =  129.3401596918702\n",
      "----------\n",
      "Epoch 11/100\n",
      "Loss =  125.02821899950504\n",
      "----------\n",
      "Epoch 12/100\n",
      "Loss =  129.1924626454711\n",
      "----------\n",
      "Epoch 13/100\n",
      "Loss =  128.9174106195569\n",
      "----------\n",
      "Epoch 14/100\n",
      "Loss =  126.23922093212605\n",
      "----------\n",
      "Epoch 15/100\n",
      "Loss =  125.11551930382848\n",
      "----------\n",
      "Epoch 16/100\n",
      "Loss =  126.57033047825098\n",
      "----------\n",
      "Epoch 17/100\n",
      "Loss =  125.95798751711845\n",
      "----------\n",
      "Epoch 18/100\n",
      "Loss =  124.98565695807338\n",
      "----------\n",
      "Epoch 19/100\n",
      "Loss =  126.9237317070365\n",
      "----------\n",
      "Epoch 20/100\n",
      "Loss =  127.13050310313702\n",
      "----------\n",
      "Epoch 21/100\n",
      "Loss =  126.44736322760582\n",
      "----------\n",
      "Epoch 22/100\n",
      "Loss =  128.86656536906958\n",
      "----------\n",
      "Epoch 23/100\n",
      "Loss =  124.78176400065422\n",
      "----------\n",
      "Epoch 24/100\n",
      "Loss =  127.53020231425762\n",
      "----------\n",
      "Epoch 25/100\n",
      "Loss =  130.70112968981266\n",
      "----------\n",
      "Epoch 26/100\n",
      "Loss =  127.14951038360596\n",
      "----------\n",
      "Epoch 27/100\n",
      "Loss =  125.4844387024641\n",
      "----------\n",
      "Epoch 28/100\n",
      "Loss =  122.20766384527087\n",
      "----------\n",
      "Epoch 29/100\n",
      "Loss =  131.85019700229168\n",
      "----------\n",
      "Epoch 30/100\n",
      "Loss =  127.40024871379137\n",
      "----------\n",
      "Epoch 31/100\n",
      "Loss =  129.64269805699587\n",
      "----------\n",
      "Epoch 32/100\n",
      "Loss =  128.1719083711505\n",
      "----------\n",
      "Epoch 33/100\n",
      "Loss =  121.2967427521944\n",
      "----------\n",
      "Epoch 34/100\n",
      "Loss =  126.30928587168455\n",
      "----------\n",
      "Epoch 35/100\n",
      "Loss =  125.4182865768671\n",
      "----------\n",
      "Epoch 36/100\n",
      "Loss =  123.54740522801876\n",
      "----------\n",
      "Epoch 37/100\n",
      "Loss =  121.23888424038887\n",
      "----------\n",
      "Epoch 38/100\n",
      "Loss =  143.47367165610194\n",
      "----------\n",
      "Epoch 39/100\n",
      "Loss =  128.28628835827112\n",
      "----------\n",
      "Epoch 40/100\n",
      "Loss =  126.80992690473795\n",
      "----------\n",
      "Epoch 41/100\n",
      "Loss =  127.61794912815094\n",
      "----------\n",
      "Epoch 42/100\n",
      "Loss =  125.48146419972181\n",
      "----------\n",
      "Epoch 43/100\n",
      "Loss =  125.12924164533615\n",
      "----------\n",
      "Epoch 44/100\n",
      "Loss =  126.03378219902515\n",
      "----------\n",
      "Epoch 45/100\n",
      "Loss =  127.71488742530346\n",
      "----------\n",
      "Epoch 46/100\n",
      "Loss =  124.8491215184331\n",
      "----------\n",
      "Epoch 47/100\n",
      "Loss =  121.62055587768555\n",
      "----------\n",
      "Epoch 48/100\n",
      "Loss =  128.7124840915203\n",
      "----------\n",
      "Epoch 49/100\n",
      "Loss =  128.76156590133905\n",
      "----------\n",
      "Epoch 50/100\n",
      "Loss =  124.36761195212603\n",
      "----------\n",
      "Epoch 51/100\n",
      "Loss =  123.29840741306543\n",
      "----------\n",
      "Epoch 52/100\n",
      "Loss =  120.34359292685986\n",
      "----------\n",
      "Epoch 53/100\n",
      "Loss =  122.53527996689081\n",
      "----------\n",
      "Epoch 54/100\n",
      "Loss =  122.01068240404129\n",
      "----------\n",
      "Epoch 55/100\n",
      "Loss =  123.39752572774887\n",
      "----------\n",
      "Epoch 56/100\n",
      "Loss =  123.08325002342463\n",
      "----------\n",
      "Epoch 57/100\n",
      "Loss =  120.50319913774729\n",
      "----------\n",
      "Epoch 58/100\n",
      "Loss =  124.5913759842515\n",
      "----------\n",
      "Epoch 59/100\n",
      "Loss =  123.67842354625463\n",
      "----------\n",
      "Epoch 60/100\n",
      "Loss =  124.87331924587488\n",
      "----------\n",
      "Epoch 61/100\n",
      "Loss =  124.18761017173529\n",
      "----------\n",
      "Epoch 62/100\n",
      "Loss =  125.12973771989346\n",
      "----------\n",
      "Epoch 63/100\n",
      "Loss =  126.77838058769703\n",
      "----------\n",
      "Epoch 64/100\n",
      "Loss =  126.34897588938475\n",
      "----------\n",
      "Epoch 65/100\n",
      "Loss =  122.73799551278353\n",
      "----------\n",
      "Epoch 66/100\n",
      "Loss =  120.32880765199661\n",
      "----------\n",
      "Epoch 67/100\n",
      "Loss =  126.56099930405617\n",
      "----------\n",
      "Epoch 68/100\n",
      "Loss =  120.22009737044573\n",
      "----------\n",
      "Epoch 69/100\n",
      "Loss =  120.45302236825228\n",
      "----------\n",
      "Epoch 70/100\n",
      "Loss =  121.41236241161823\n",
      "----------\n",
      "Epoch 71/100\n",
      "Loss =  121.30384982377291\n",
      "----------\n",
      "Epoch 72/100\n",
      "Loss =  123.61923252791166\n",
      "----------\n",
      "Epoch 73/100\n",
      "Loss =  123.99534736573696\n",
      "----------\n",
      "Epoch 74/100\n",
      "Loss =  119.38019672781229\n",
      "----------\n",
      "Epoch 75/100\n",
      "Loss =  125.79317601025105\n",
      "----------\n",
      "Epoch 76/100\n",
      "Loss =  121.9299393221736\n",
      "----------\n",
      "Epoch 77/100\n",
      "Loss =  124.28995555639267\n",
      "----------\n",
      "Epoch 78/100\n",
      "Loss =  122.07127030938864\n",
      "----------\n",
      "Epoch 79/100\n",
      "Loss =  123.12930826097727\n",
      "----------\n",
      "Epoch 80/100\n",
      "Loss =  120.16361762583256\n",
      "----------\n",
      "Epoch 81/100\n",
      "Loss =  119.99708374589682\n",
      "----------\n",
      "Epoch 82/100\n",
      "Loss =  123.70015205442905\n",
      "----------\n",
      "Epoch 83/100\n",
      "Loss =  118.3102364987135\n",
      "----------\n",
      "Epoch 84/100\n",
      "Loss =  123.19497084617615\n",
      "----------\n",
      "Epoch 85/100\n",
      "Loss =  121.05664762109518\n",
      "----------\n",
      "Epoch 86/100\n",
      "Loss =  123.13255359232426\n",
      "----------\n",
      "Epoch 87/100\n",
      "Loss =  121.73719667643309\n",
      "----------\n",
      "Epoch 88/100\n",
      "Loss =  120.4864115267992\n",
      "----------\n",
      "Epoch 89/100\n",
      "Loss =  121.69363743811846\n",
      "----------\n",
      "Epoch 90/100\n",
      "Loss =  117.64009257405996\n",
      "----------\n",
      "Epoch 91/100\n",
      "Loss =  130.59510935097933\n",
      "----------\n",
      "Epoch 92/100\n",
      "Loss =  121.89164371788502\n",
      "----------\n",
      "Epoch 93/100\n",
      "Loss =  121.34689174592495\n",
      "----------\n",
      "Epoch 94/100\n",
      "Loss =  122.13109332323074\n",
      "----------\n",
      "Epoch 95/100\n",
      "Loss =  128.31191943213344\n",
      "----------\n",
      "Epoch 96/100\n",
      "Loss =  121.51894724369049\n",
      "----------\n",
      "Epoch 97/100\n",
      "Loss =  123.3294040337205\n",
      "----------\n",
      "Epoch 98/100\n",
      "Loss =  123.47626382857561\n",
      "----------\n",
      "Epoch 99/100\n",
      "Loss =  123.2170339524746\n",
      "----------\n",
      "Epoch 100/100\n",
      "Loss =  122.52677316591144\n",
      "----------\n",
      "Epoch 1/100\n",
      "Loss =  165.51292868983\n",
      "----------\n",
      "Epoch 2/100\n",
      "Loss =  270.4115702137351\n",
      "----------\n",
      "Epoch 3/100\n",
      "Loss =  207.60826209944207\n",
      "----------\n",
      "Epoch 4/100\n",
      "Loss =  180.2202161657624\n",
      "----------\n",
      "Epoch 5/100\n",
      "Loss =  163.66342199407518\n",
      "----------\n",
      "Epoch 6/100\n",
      "Loss =  170.24717657314613\n",
      "----------\n",
      "Epoch 7/100\n",
      "Loss =  210.49532808730146\n",
      "----------\n",
      "Epoch 8/100\n",
      "Loss =  199.88287465064786\n",
      "----------\n",
      "Epoch 9/100\n",
      "Loss =  233.68209742050385\n",
      "----------\n",
      "Epoch 10/100\n",
      "Loss =  185.4076330207754\n",
      "----------\n",
      "Epoch 11/100\n",
      "Loss =  173.78177576667804\n",
      "----------\n",
      "Epoch 12/100\n",
      "Loss =  282.0332594289712\n",
      "----------\n",
      "Epoch 13/100\n",
      "Loss =  253.57784845695278\n",
      "----------\n",
      "Epoch 14/100\n",
      "Loss =  261.75855297149246\n",
      "----------\n",
      "Epoch 15/100\n",
      "Loss =  284.4828133068859\n",
      "----------\n",
      "Epoch 16/100\n",
      "Loss =  193.35843296023086\n",
      "----------\n",
      "Epoch 17/100\n",
      "Loss =  155.1502964189276\n",
      "----------\n",
      "Epoch 18/100\n",
      "Loss =  265.57312804226603\n",
      "----------\n",
      "Epoch 19/100\n",
      "Loss =  255.51041591977992\n",
      "----------\n",
      "Epoch 20/100\n",
      "Loss =  189.3366804192774\n",
      "----------\n",
      "Epoch 21/100\n",
      "Loss =  217.69712836213876\n",
      "----------\n",
      "Epoch 22/100\n",
      "Loss =  270.28886414488807\n",
      "----------\n",
      "Epoch 23/100\n",
      "Loss =  251.11078427528264\n",
      "----------\n",
      "Epoch 24/100\n",
      "Loss =  180.9645122361253\n",
      "----------\n",
      "Epoch 25/100\n",
      "Loss =  194.51543213854893\n",
      "----------\n",
      "Epoch 26/100\n",
      "Loss =  180.81805217400688\n",
      "----------\n",
      "Epoch 27/100\n",
      "Loss =  245.16861580840305\n",
      "----------\n",
      "Epoch 28/100\n",
      "Loss =  188.3078807653801\n",
      "----------\n",
      "Epoch 29/100\n",
      "Loss =  171.48776350915432\n",
      "----------\n",
      "Epoch 30/100\n",
      "Loss =  165.9966034090612\n",
      "----------\n",
      "Epoch 31/100\n",
      "Loss =  196.94961309197242\n",
      "----------\n",
      "Epoch 32/100\n",
      "Loss =  245.13129236910515\n",
      "----------\n",
      "Epoch 33/100\n",
      "Loss =  166.23435095930472\n",
      "----------\n",
      "Epoch 34/100\n",
      "Loss =  173.16717197301477\n",
      "----------\n",
      "Epoch 35/100\n",
      "Loss =  143.84809147450142\n",
      "----------\n",
      "Epoch 36/100\n",
      "Loss =  181.87529827217804\n",
      "----------\n",
      "Epoch 37/100\n",
      "Loss =  210.95109983357816\n",
      "----------\n",
      "Epoch 38/100\n",
      "Loss =  269.03508830488147\n",
      "----------\n",
      "Epoch 39/100\n",
      "Loss =  159.44186485306273\n",
      "----------\n",
      "Epoch 40/100\n",
      "Loss =  237.15979791775317\n",
      "----------\n",
      "Epoch 41/100\n",
      "Loss =  190.56155748641686\n",
      "----------\n",
      "Epoch 42/100\n",
      "Loss =  198.45943496681866\n",
      "----------\n",
      "Epoch 43/100\n",
      "Loss =  177.47592652542517\n",
      "----------\n",
      "Epoch 44/100\n",
      "Loss =  167.1670364929014\n",
      "----------\n",
      "Epoch 45/100\n",
      "Loss =  168.87278758734465\n",
      "----------\n",
      "Epoch 46/100\n",
      "Loss =  146.73809206392616\n",
      "----------\n",
      "Epoch 47/100\n",
      "Loss =  159.01301434542984\n",
      "----------\n",
      "Epoch 48/100\n",
      "Loss =  166.01953469775617\n",
      "----------\n",
      "Epoch 49/100\n",
      "Loss =  164.68352943117497\n",
      "----------\n",
      "Epoch 50/100\n",
      "Loss =  200.2749542585807\n",
      "----------\n",
      "Epoch 51/100\n",
      "Loss =  185.0300282589742\n",
      "----------\n",
      "Epoch 52/100\n",
      "Loss =  152.0185325447237\n",
      "----------\n",
      "Epoch 53/100\n",
      "Loss =  171.9619959719712\n",
      "----------\n",
      "Epoch 54/100\n",
      "Loss =  181.17449548149978\n",
      "----------\n",
      "Epoch 55/100\n",
      "Loss =  160.9560460858047\n",
      "----------\n",
      "Epoch 56/100\n",
      "Loss =  146.77835212554783\n",
      "----------\n",
      "Epoch 57/100\n",
      "Loss =  149.147526058543\n",
      "----------\n",
      "Epoch 58/100\n",
      "Loss =  216.76671547317684\n",
      "----------\n",
      "Epoch 59/100\n",
      "Loss =  195.12819010634848\n",
      "----------\n",
      "Epoch 60/100\n",
      "Loss =  144.97140011144802\n",
      "----------\n",
      "Epoch 61/100\n",
      "Loss =  173.4523425862717\n",
      "----------\n",
      "Epoch 62/100\n",
      "Loss =  203.0542071979703\n",
      "----------\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  153.34858786629047\n",
      "----------\n",
      "Epoch 64/100\n",
      "Loss =  274.6786534490593\n",
      "----------\n",
      "Epoch 65/100\n",
      "Loss =  183.92653231541044\n",
      "----------\n",
      "Epoch 66/100\n",
      "Loss =  165.26961351686623\n",
      "----------\n",
      "Epoch 67/100\n",
      "Loss =  195.7816793481761\n",
      "----------\n",
      "Epoch 68/100\n",
      "Loss =  242.5773162808764\n",
      "----------\n",
      "Epoch 69/100\n",
      "Loss =  152.56804162473418\n",
      "----------\n",
      "Epoch 70/100\n",
      "Loss =  172.7105991261924\n",
      "----------\n",
      "Epoch 71/100\n",
      "Loss =  159.86490773578407\n",
      "----------\n",
      "Epoch 72/100\n",
      "Loss =  180.21159140852978\n",
      "----------\n",
      "Epoch 73/100\n",
      "Loss =  188.34322062718275\n",
      "----------\n",
      "Epoch 74/100\n",
      "Loss =  163.93510079174303\n",
      "----------\n",
      "Epoch 75/100\n",
      "Loss =  203.81542098184946\n",
      "----------\n",
      "Epoch 76/100\n",
      "Loss =  177.005865589701\n",
      "----------\n",
      "Epoch 77/100\n",
      "Loss =  218.48187055025755\n",
      "----------\n",
      "Epoch 78/100\n",
      "Loss =  156.69128044939134\n",
      "----------\n",
      "Epoch 79/100\n",
      "Loss =  128.62211294379085\n",
      "----------\n",
      "Epoch 80/100\n",
      "Loss =  180.45763109921245\n",
      "----------\n",
      "Epoch 81/100\n",
      "Loss =  192.11106833425583\n",
      "----------\n",
      "Epoch 82/100\n",
      "Loss =  161.86934048587864\n",
      "----------\n",
      "Epoch 83/100\n",
      "Loss =  168.80609070393257\n",
      "----------\n",
      "Epoch 84/100\n",
      "Loss =  154.0534033565782\n",
      "----------\n",
      "Epoch 85/100\n",
      "Loss =  224.94126394998943\n",
      "----------\n",
      "Epoch 86/100\n",
      "Loss =  133.5373913999647\n",
      "----------\n",
      "Epoch 87/100\n",
      "Loss =  174.5509274287033\n",
      "----------\n",
      "Epoch 88/100\n",
      "Loss =  149.20164246525383\n",
      "----------\n",
      "Epoch 89/100\n",
      "Loss =  275.0924696745151\n",
      "----------\n",
      "Epoch 90/100\n",
      "Loss =  167.98548119631596\n",
      "----------\n",
      "Epoch 91/100\n",
      "Loss =  219.2281181976814\n",
      "----------\n",
      "Epoch 92/100\n",
      "Loss =  186.3713208771078\n",
      "----------\n",
      "Epoch 93/100\n",
      "Loss =  184.14697695476934\n",
      "----------\n",
      "Epoch 94/100\n",
      "Loss =  202.5248870422547\n",
      "----------\n",
      "Epoch 95/100\n",
      "Loss =  217.50203567114715\n",
      "----------\n",
      "Epoch 96/100\n",
      "Loss =  168.58365134356427\n",
      "----------\n",
      "Epoch 97/100\n",
      "Loss =  215.1187985635479\n",
      "----------\n",
      "Epoch 98/100\n",
      "Loss =  158.06795774507918\n",
      "----------\n",
      "Epoch 99/100\n",
      "Loss =  177.7407620918675\n",
      "----------\n",
      "Epoch 100/100\n",
      "Loss =  179.51787039744886\n",
      "----------\n",
      "Epoch 1/100\n",
      "Loss =  3572.0174608231214\n",
      "----------\n",
      "Epoch 2/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 3/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 4/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 5/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 6/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 7/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 8/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 9/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 10/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 11/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 12/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 13/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 14/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 15/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 16/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 17/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 18/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 19/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 20/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 21/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 22/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 23/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 24/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 25/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 26/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 27/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 28/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 29/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 30/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 31/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 32/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 33/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 34/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 35/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 36/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 37/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 38/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 39/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 40/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 41/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 42/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 43/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 44/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 45/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 46/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 47/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 48/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 49/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 50/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 51/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 52/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 53/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 54/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 55/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 56/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 57/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 58/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 59/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 60/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 61/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 62/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 63/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 64/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 65/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 66/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 67/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 68/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 69/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 70/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 71/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 72/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 73/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 74/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 75/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 76/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 77/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 78/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 79/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 80/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 81/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 82/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 83/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 84/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 85/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 86/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 87/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 88/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 89/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 90/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 91/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 92/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 93/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 94/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 95/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 96/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 97/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 98/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 99/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n",
      "Epoch 100/100\n",
      "Loss =  3840.7119884490967\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "# turn off gradient update for old layers\n",
    "# for param in myModel.parameters():\n",
    "#     param.requires_grad = False\n",
    "# myModel.conv3_3_norm_gender.weight.requires_grad = True\n",
    "# myModel.conv3_3_norm_gender.bias.requires_grad = True\n",
    "\n",
    "# myModel.fc_1 = nn.Linear(2304,num_classes)\n",
    "# for parameter in myModel.parameters():\n",
    "#     print(parameter.requires_grad)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.0001, momentum=0.9)\n",
    "optimizer2 = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.001, momentum=0.9)\n",
    "optimizer3 = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.01, momentum=0.9)\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"using cuda\")\n",
    "    myModel = myModel.cuda()\n",
    "model_0001 = train_model(myModel, criterion, optimizer, num_classes, num_epochs=100)\n",
    "model_001 = train_model(myModel, criterion, optimizer2, num_classes, num_epochs=100)\n",
    "model_01 = train_model(myModel, criterion, optimizer3, num_classes, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-09104a22773a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3358\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/deep_learning/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 242\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testImage1 -  tensor([[ 0.2463,  0.7537]], device='cuda:0')\n",
      "testImage2 -  tensor([[ 0.4145,  0.5855]], device='cuda:0')\n",
      "testImage3 -  tensor([[ 0.4145,  0.5855]], device='cuda:0')\n",
      "testImage1 -  tensor([[ 0.3995,  0.6005]], device='cuda:0')\n",
      "testImage2 -  tensor([[ 0.3419,  0.6581]], device='cuda:0')\n",
      "testImage3 -  tensor([[ 0.1945,  0.8055]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def transform(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        return Variable(img.cuda())\n",
    "myModel = myModel.cuda()\n",
    "testImage1 = transform('data/Test/TestCeleb_4/25-FaceId-0.jpg')\n",
    "testImage2 = transform('data/Test/TestCeleb_4/26-FaceId-0.jpg')\n",
    "testImage3 = transform('data/Test/TestCeleb_4/27-FaceId-0.jpg')\n",
    "testImage4 = transform('data/Test/TestCeleb_10/25-FaceId-0.jpg')\n",
    "testImage5 = transform('data/Test/TestCeleb_10/26-FaceId-0.jpg')\n",
    "testImage6 = transform('data/Test/TestCeleb_8/26-FaceId-0.jpg')\n",
    "\n",
    "output1 = myModel(testImage1)\n",
    "output2 = myModel(testImage2)\n",
    "output3 = myModel(testImage2)\n",
    "output4 = myModel(testImage4)\n",
    "output5 = myModel(testImage5)\n",
    "output6 = myModel(testImage6)\n",
    "print(\"testImage1 - \",output1)\n",
    "print(\"testImage2 - \",output2)\n",
    "print(\"testImage3 - \",output3)\n",
    "print(\"testImage1 - \",output4)\n",
    "print(\"testImage2 - \",output5)\n",
    "print(\"testImage3 - \",output6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning]",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
